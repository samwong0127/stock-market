{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOgZ-f5ChPMq"
      },
      "outputs": [],
      "source": [
        "!unzip /content/drive/MyDrive/StockMarket/archive.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jbZc_LgSjHWs"
      },
      "outputs": [],
      "source": [
        "# Logging\n",
        "import logging\n",
        "from datetime import date\n",
        "import os\n",
        "#print(os.getcwd())\n",
        "today = date.today()\n",
        "filename = str(today)+\"-pipeline.log\"\n",
        "#filename = os.path.join(os.getcwd(), str(today)+\"-pipeline.log\")\n",
        "logging.basicConfig(filename=filename, \n",
        "                    filemode='w', \n",
        "                    level=logging.DEBUG, \n",
        "                    format='%(asctime)s, %(name)s %(levelname)s: %(message)s')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCsmmziYKsdV"
      },
      "source": [
        "### Data Ingestion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KoqLRDV83_w1"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "# Get a list of all CSV files in the directory\n",
        "etf_list = glob.glob(\"etfs/*.csv\")\n",
        "#print(etf_list)\n",
        "logging.info(f'A total of {len(etf_list)} ETFs found.')\n",
        "\n",
        "stock_list = glob.glob(\"stocks/*.csv\")\n",
        "#print(stock_list)\n",
        "logging.info(f'A total of {len(stock_list)} stocks found.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8HdHwhjnH_o",
        "outputId": "f5aa770a-20f9-4e48-86ae-05d1c33d231f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317145 sha256=7ea961c5bbdc9dd222ada3c77dac075d1842ba44e0b8bffe5b7ebc1faf77e34a\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEVAvkhynCUs"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import input_file_name\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"pipeline\").getOrCreate()\n",
        "\n",
        "# Read all CSV files as a Spark DataFrame and add a new column with the file name\n",
        "df = spark.read.csv(etf_list, header=True).withColumn(\"Symbol\", input_file_name())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.union(spark.read.csv(stock_list, header=True).withColumn(\"Symbol\", input_file_name()))\n",
        "\n",
        "print(f'Rows: {df.count()}, Columns: {len(df.columns)} combined.')\n",
        "logging.info('Combined all CSV files into a Spark DataFrame')\n",
        "logging.info(f'With Rows: {df.count()}, Columns: {len(df.columns)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1SgjR6z7c3g",
        "outputId": "21968c6a-a803-423e-9a0f-00349e851461"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows: 28151758, Columns: 8 combined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3Yd9M1hng8I",
        "outputId": "c508757c-c85a-40ae-9d1b-7832b0a9bc4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+----+------+------+------+-----------------+------+--------------------+\n",
            "|      Date|Open|  High|   Low| Close|        Adj Close|Volume|              Symbol|\n",
            "+----------+----+------+------+------+-----------------+------+--------------------+\n",
            "|1986-04-03| 0.0|  4.75| 4.625| 4.625|4.449552059173584| 15300|file:/content/etf...|\n",
            "|1986-04-04| 0.0|  4.75|4.6875|  4.75| 4.56981086730957| 12000|file:/content/etf...|\n",
            "|1986-04-07| 0.0| 4.875|  4.75|  4.75| 4.56981086730957| 11500|file:/content/etf...|\n",
            "|1986-04-08| 0.0|4.8125|4.6875|  4.75| 4.56981086730957| 21000|file:/content/etf...|\n",
            "|1986-04-09| 0.0|4.8125| 4.625|4.6875| 4.50968074798584| 22800|file:/content/etf...|\n",
            "|1986-04-10| 0.0|4.6875| 4.625| 4.625|4.449552059173584|  6200|file:/content/etf...|\n",
            "|1986-04-11| 0.0|4.6875|4.5625| 4.625|4.449552059173584| 37100|file:/content/etf...|\n",
            "|1986-04-14| 0.0| 4.625|   4.5|4.5625|4.389422416687012| 28200|file:/content/etf...|\n",
            "|1986-04-15| 0.0|4.6875|4.5625| 4.625|4.449552059173584| 14200|file:/content/etf...|\n",
            "|1986-04-16| 0.0|4.5625|   4.5|4.5625|4.389422416687012| 14300|file:/content/etf...|\n",
            "|1986-04-17| 0.0| 4.625|4.4375|   4.5|4.329293727874756| 23400|file:/content/etf...|\n",
            "|1986-04-18| 0.0|   4.5|4.4375|4.4375|4.269164562225342| 13800|file:/content/etf...|\n",
            "|1986-04-21| 0.0| 4.375|  4.25| 4.375|4.209035873413086| 28200|file:/content/etf...|\n",
            "|1986-04-22| 0.0| 4.375|4.3125| 4.375|4.209035873413086|  8800|file:/content/etf...|\n",
            "|1986-04-23| 0.0|   4.5| 4.375|   4.5|4.329293727874756| 11800|file:/content/etf...|\n",
            "|1986-04-24| 0.0|4.5625|   4.5|4.5625|4.389422416687012|  4300|file:/content/etf...|\n",
            "|1986-04-25| 0.0|4.5625|   4.5|4.5625|4.389422416687012| 12400|file:/content/etf...|\n",
            "|1986-04-28| 0.0|4.5625|   4.5|   4.5|4.329293727874756|  4200|file:/content/etf...|\n",
            "|1986-04-29| 0.0|   4.5|4.4375|4.4375|4.269164562225342|  5600|file:/content/etf...|\n",
            "|1986-04-30| 0.0|   4.5| 4.375|   4.5|4.329293727874756| 39400|file:/content/etf...|\n",
            "+----------+----+------+------+------+-----------------+------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KglRYHkCnRtG"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import regexp_extract\n",
        "\n",
        "# Extract the filename from the \"Symbol\" column in the original DataFrame (df)\n",
        "df = df.withColumn(\"Symbol\", regexp_extract(df[\"Symbol\"], r\"([^/]+)\\.csv$\", 1))\n",
        "#df.show()\n",
        "logging.info('Extracted Symbol from path')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFYgKFEY9kVl",
        "outputId": "9931a5a1-6c6f-4179-83fc-568359c8eadd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Date: string (nullable = true)\n",
            " |-- Open: string (nullable = true)\n",
            " |-- High: string (nullable = true)\n",
            " |-- Low: string (nullable = true)\n",
            " |-- Close: string (nullable = true)\n",
            " |-- Adj Close: string (nullable = true)\n",
            " |-- Volume: string (nullable = true)\n",
            " |-- Symbol: string (nullable = false)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sf96uShL9Ujq",
        "outputId": "8ec1cbde-65f8-40dd-84f7-e5a5bdaadd7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Date: string (nullable = true)\n",
            " |-- Open: float (nullable = true)\n",
            " |-- High: float (nullable = true)\n",
            " |-- Low: float (nullable = true)\n",
            " |-- Close: float (nullable = true)\n",
            " |-- Adj Close: float (nullable = true)\n",
            " |-- Volume: integer (nullable = true)\n",
            " |-- Symbol: string (nullable = false)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "df = df.withColumn(\"Open\", col('Open').cast('float')) \\\n",
        "    .withColumn(\"High\", col('High').cast('float')) \\\n",
        "    .withColumn(\"Low\", col('Low').cast('float')) \\\n",
        "    .withColumn(\"Close\", col('Close').cast('float')) \\\n",
        "    .withColumn(\"Adj Close\", col('Adj Close').cast('float')) \\\n",
        "    .withColumn(\"Volume\", col('Volume').cast('int'))\n",
        "df.printSchema()\n",
        "logging.info('Changed the data type of specific columns')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gz0q9ifMnowQ",
        "outputId": "37769095-cd99-4327-8f9d-1e55640f869a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------------+\n",
            "|Symbol|       Security Name|\n",
            "+------+--------------------+\n",
            "|     A|Agilent Technolog...|\n",
            "|    AA|Alcoa Corporation...|\n",
            "|  AAAU|Perth Mint Physic...|\n",
            "|  AACG|ATA Creativity Gl...|\n",
            "|  AADR|AdvisorShares Dor...|\n",
            "|   AAL|American Airlines...|\n",
            "|  AAMC|Altisource Asset ...|\n",
            "|  AAME|Atlantic American...|\n",
            "|   AAN|Aaron's, Inc. Com...|\n",
            "|  AAOI|Applied Optoelect...|\n",
            "|  AAON|AAON, Inc. - Comm...|\n",
            "|   AAP|Advance Auto Part...|\n",
            "|  AAPL|Apple Inc. - Comm...|\n",
            "|   AAT|American Assets T...|\n",
            "|   AAU|Almaden Minerals,...|\n",
            "|  AAWW|Atlas Air Worldwi...|\n",
            "|  AAXJ|iShares MSCI All ...|\n",
            "|  AAXN|Axon Enterprise, ...|\n",
            "|    AB|AllianceBernstein...|\n",
            "|   ABB|ABB Ltd Common Stock|\n",
            "+------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Read the metadata CSV file into a Spark DataFrame and select only the relevant columns\n",
        "metadata_df = spark.read.csv(\"symbols_valid_meta.csv\", header=True)\n",
        "\n",
        "metadata_df = metadata_df.select(\"Symbol\", \"Security Name\")\n",
        "metadata_df.show()\n",
        "logging.info('Read metadata CSV file')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcWqzlvLnt8d",
        "outputId": "7c794885-aabf-4fe3-e04c-49c212410a88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----------+----+------+------+------+---------+------+--------------------+\n",
            "|Symbol|      Date|Open|  High|   Low| Close|Adj Close|Volume|       Security Name|\n",
            "+------+----------+----+------+------+------+---------+------+--------------------+\n",
            "|   CEF|1986-04-03| 0.0|  4.75| 4.625| 4.625| 4.449552| 15300|Sprott Physical G...|\n",
            "|   CEF|1986-04-04| 0.0|  4.75|4.6875|  4.75| 4.569811| 12000|Sprott Physical G...|\n",
            "|   CEF|1986-04-07| 0.0| 4.875|  4.75|  4.75| 4.569811| 11500|Sprott Physical G...|\n",
            "|   CEF|1986-04-08| 0.0|4.8125|4.6875|  4.75| 4.569811| 21000|Sprott Physical G...|\n",
            "|   CEF|1986-04-09| 0.0|4.8125| 4.625|4.6875|4.5096807| 22800|Sprott Physical G...|\n",
            "|   CEF|1986-04-10| 0.0|4.6875| 4.625| 4.625| 4.449552|  6200|Sprott Physical G...|\n",
            "|   CEF|1986-04-11| 0.0|4.6875|4.5625| 4.625| 4.449552| 37100|Sprott Physical G...|\n",
            "|   CEF|1986-04-14| 0.0| 4.625|   4.5|4.5625|4.3894224| 28200|Sprott Physical G...|\n",
            "|   CEF|1986-04-15| 0.0|4.6875|4.5625| 4.625| 4.449552| 14200|Sprott Physical G...|\n",
            "|   CEF|1986-04-16| 0.0|4.5625|   4.5|4.5625|4.3894224| 14300|Sprott Physical G...|\n",
            "+------+----------+----+------+------+------+---------+------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "root\n",
            " |-- Symbol: string (nullable = false)\n",
            " |-- Date: string (nullable = true)\n",
            " |-- Open: float (nullable = true)\n",
            " |-- High: float (nullable = true)\n",
            " |-- Low: float (nullable = true)\n",
            " |-- Close: float (nullable = true)\n",
            " |-- Adj Close: float (nullable = true)\n",
            " |-- Volume: integer (nullable = true)\n",
            " |-- Security Name: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Join the original DataFrame (df) with the metadata DataFrame on the \"Symbol\" column\n",
        "df = df.join(metadata_df, on=[\"Symbol\"], how=\"left\")\n",
        "df.show(10)\n",
        "df.printSchema()\n",
        "logging.info('Joined Security Name from metadata to combined DataFrame')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4MPN1dpyuIW",
        "outputId": "f03d263c-fb6f-471e-acdd-3cd2a49ecd51"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Symbol',\n",
              " 'Date',\n",
              " 'Open',\n",
              " 'High',\n",
              " 'Low',\n",
              " 'Close',\n",
              " 'Adj Close',\n",
              " 'Volume',\n",
              " 'Security Name']"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-YFWfBVWYpe"
      },
      "source": [
        "#### Rearrange the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QycYQ432zEeY",
        "outputId": "ba0b51bb-8a78-4f2f-faaf-f16909629eee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------------+----------+----+------+------+------+---------+------+\n",
            "|Symbol|       Security Name|      Date|Open|  High|   Low| Close|Adj Close|Volume|\n",
            "+------+--------------------+----------+----+------+------+------+---------+------+\n",
            "|   CEF|Sprott Physical G...|1986-04-03| 0.0|  4.75| 4.625| 4.625| 4.449552| 15300|\n",
            "|   CEF|Sprott Physical G...|1986-04-04| 0.0|  4.75|4.6875|  4.75| 4.569811| 12000|\n",
            "|   CEF|Sprott Physical G...|1986-04-07| 0.0| 4.875|  4.75|  4.75| 4.569811| 11500|\n",
            "|   CEF|Sprott Physical G...|1986-04-08| 0.0|4.8125|4.6875|  4.75| 4.569811| 21000|\n",
            "|   CEF|Sprott Physical G...|1986-04-09| 0.0|4.8125| 4.625|4.6875|4.5096807| 22800|\n",
            "|   CEF|Sprott Physical G...|1986-04-10| 0.0|4.6875| 4.625| 4.625| 4.449552|  6200|\n",
            "|   CEF|Sprott Physical G...|1986-04-11| 0.0|4.6875|4.5625| 4.625| 4.449552| 37100|\n",
            "|   CEF|Sprott Physical G...|1986-04-14| 0.0| 4.625|   4.5|4.5625|4.3894224| 28200|\n",
            "|   CEF|Sprott Physical G...|1986-04-15| 0.0|4.6875|4.5625| 4.625| 4.449552| 14200|\n",
            "|   CEF|Sprott Physical G...|1986-04-16| 0.0|4.5625|   4.5|4.5625|4.3894224| 14300|\n",
            "|   CEF|Sprott Physical G...|1986-04-17| 0.0| 4.625|4.4375|   4.5|4.3292937| 23400|\n",
            "|   CEF|Sprott Physical G...|1986-04-18| 0.0|   4.5|4.4375|4.4375|4.2691646| 13800|\n",
            "|   CEF|Sprott Physical G...|1986-04-21| 0.0| 4.375|  4.25| 4.375| 4.209036| 28200|\n",
            "|   CEF|Sprott Physical G...|1986-04-22| 0.0| 4.375|4.3125| 4.375| 4.209036|  8800|\n",
            "|   CEF|Sprott Physical G...|1986-04-23| 0.0|   4.5| 4.375|   4.5|4.3292937| 11800|\n",
            "|   CEF|Sprott Physical G...|1986-04-24| 0.0|4.5625|   4.5|4.5625|4.3894224|  4300|\n",
            "|   CEF|Sprott Physical G...|1986-04-25| 0.0|4.5625|   4.5|4.5625|4.3894224| 12400|\n",
            "|   CEF|Sprott Physical G...|1986-04-28| 0.0|4.5625|   4.5|   4.5|4.3292937|  4200|\n",
            "|   CEF|Sprott Physical G...|1986-04-29| 0.0|   4.5|4.4375|4.4375|4.2691646|  5600|\n",
            "|   CEF|Sprott Physical G...|1986-04-30| 0.0|   4.5| 4.375|   4.5|4.3292937| 39400|\n",
            "+------+--------------------+----------+----+------+------+------+---------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = df.select('Symbol','Security Name',\n",
        " 'Date',\n",
        " 'Open',\n",
        " 'High',\n",
        " 'Low',\n",
        " 'Close',\n",
        " 'Adj Close',\n",
        " 'Volume')\n",
        "df.show()\n",
        "logging.info('Rearranged columns')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7Ug0iovynxFc"
      },
      "outputs": [],
      "source": [
        "\n",
        "df = df.withColumnRenamed(\"Security Name\", \"Security_Name\")\n",
        "df = df.withColumnRenamed(\"Adj Close\", \"Adj_Close\")\n",
        "#df.show()\n",
        "logging.info('Renamed columns to remove spaces')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xL_6mqzdLHzd"
      },
      "source": [
        "#### Formatting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iuwRPk9n7ld"
      },
      "outputs": [],
      "source": [
        "# Save as CSV\n",
        "# Try to use coalesce() instead of repartition() to reduce the number of partitions\n",
        "df.repartition(1).write.csv(\"etfs.csv\", header=True, mode=\"overwrite\")\n",
        "logging.info('Saved current DataFrame as a CSV file')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yq0N7Ac8rUaB"
      },
      "outputs": [],
      "source": [
        "# Save as Parquet\n",
        "# Try to use coalesce() instead of repartition() to reduce the number of partitions\n",
        "df.write.parquet(\"etfs.parquet\", mode=\"overwrite\")\n",
        "logging.info('Saved current DataFrame as a Parquet file')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsDFsrhuqwVu"
      },
      "outputs": [],
      "source": [
        "#print((df.count(), len(df.columns)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xii1N3gdKnCQ"
      },
      "source": [
        "### Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "NCmhlNg0A2XE"
      },
      "outputs": [],
      "source": [
        "import pyspark.sql\n",
        "from pyspark.sql.functions import percentile_approx, mean\n",
        "from pyspark.sql.window import Window"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BD3RlME3tslD",
        "outputId": "ab4cf7e6-6350-4aa2-9d58-f80c03e0bda1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------------+----------+-----+-----+-----+-----+---------+--------+------------------+\n",
            "|Symbol|       Security_Name|      Date| Open| High|  Low|Close|Adj_Close|  Volume|    vol_moving_avg|\n",
            "+------+--------------------+----------+-----+-----+-----+-----+---------+--------+------------------+\n",
            "|   AAT|American Assets T...|2011-01-13|21.53| 22.0|21.18|21.25|16.332218|15536900|         1.55369E7|\n",
            "|   AAT|American Assets T...|2011-01-14|21.16|21.45|21.16|21.31|16.378332| 1304800|         8420850.0|\n",
            "|   AAT|American Assets T...|2011-01-18| 21.3|21.45| 21.2|21.37| 16.42445|  124800|         5655500.0|\n",
            "|   AAT|American Assets T...|2011-01-19|21.42|21.42|20.88|21.25|16.332218| 1010200|         4494175.0|\n",
            "|   AAT|American Assets T...|2011-01-20|21.05| 21.4|21.03|21.21|16.301474|  736600|         3742660.0|\n",
            "|   AAT|American Assets T...|2011-01-21| 21.3| 21.3|21.03|21.25|16.332218|  636800|3225016.6666666665|\n",
            "|   AAT|American Assets T...|2011-01-24| 21.2|21.39| 21.1| 21.3|16.370651|  354300| 2814914.285714286|\n",
            "|   AAT|American Assets T...|2011-01-25| 21.2|21.47|21.15|21.27|16.347591|  295700|         2500012.5|\n",
            "|   AAT|American Assets T...|2011-01-26|21.29|21.36|21.21|21.27|16.347591|  346800|2260766.6666666665|\n",
            "|   AAT|American Assets T...|2011-01-27| 21.2| 21.4| 21.2| 21.4|16.447508|  450400|         2079730.0|\n",
            "|   AAT|American Assets T...|2011-01-28|21.26|21.38|21.26| 21.3|16.370651|  475300|1933872.7272727273|\n",
            "|   AAT|American Assets T...|2011-01-31| 21.3| 21.4|21.25|21.29| 16.36296|  197600|1789183.3333333333|\n",
            "|   AAT|American Assets T...|2011-02-01|21.27| 21.4|21.21| 21.3|16.370651|  560900|         1694700.0|\n",
            "|   AAT|American Assets T...|2011-02-02|21.15|21.25|21.06|21.11|16.224619|  168300|1585671.4285714286|\n",
            "|   AAT|American Assets T...|2011-02-03|21.02|21.28|21.02|21.18| 16.27842|  549400|1516586.6666666667|\n",
            "|   AAT|American Assets T...|2011-02-04| 21.1|21.28| 21.0|21.06| 16.18619|  219600|         1435525.0|\n",
            "|   AAT|American Assets T...|2011-02-07|21.07| 21.2|20.95|21.16|16.263054|  454900|1377841.1764705882|\n",
            "|   AAT|American Assets T...|2011-02-08|21.21|21.21|20.94|20.95|16.101648|  247100|1315022.2222222222|\n",
            "|   AAT|American Assets T...|2011-02-09| 20.8| 21.2|20.61|21.14|16.247673|  423900|1268121.0526315789|\n",
            "|   AAT|American Assets T...|2011-02-10|21.01|21.35|21.01|21.24|16.324533|  256000|         1217515.0|\n",
            "+------+--------------------+----------+-----+-----+-----+-----+---------+--------+------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "root\n",
            " |-- Symbol: string (nullable = false)\n",
            " |-- Security_Name: string (nullable = true)\n",
            " |-- Date: string (nullable = true)\n",
            " |-- Open: float (nullable = true)\n",
            " |-- High: float (nullable = true)\n",
            " |-- Low: float (nullable = true)\n",
            " |-- Close: float (nullable = true)\n",
            " |-- Adj_Close: float (nullable = true)\n",
            " |-- Volume: integer (nullable = true)\n",
            " |-- vol_moving_avg: double (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "'''\n",
        "# Method 1\n",
        "df.createOrReplaceTempView(\"df_view\")\n",
        "df2 = spark.sql(\n",
        "    \"\"\"SELECT *, mean(Volume) OVER (\n",
        "        PARTITION BY Symbol \n",
        "        ORDER BY CAST(Date AS timestamp) \n",
        "        RANGE BETWEEN INTERVAL 29 DAYS PRECEDING AND CURRENT ROW\n",
        "     ) AS vol_moving_avg FROM df_view\"\"\")\n",
        "df2.show(25)\n",
        "df2.printSchema()\n",
        "'''\n",
        "# Method 2\n",
        "# Define the window specification\n",
        "windowSpec = (\n",
        "    Window()\n",
        "    .partitionBy(\"Symbol\")\n",
        "    .orderBy(col(\"Date\").cast(\"timestamp\").cast(\"long\"))\n",
        "    .rangeBetween(-29*86400, 0)\n",
        ")\n",
        "\n",
        "# Calculate the rolling 30-day median of the Adj_Close column\n",
        "df2 = df.withColumn(\"vol_moving_avg\", mean(\"Volume\").over(windowSpec))\n",
        "\n",
        "# Show the resulting DataFrame\n",
        "df2.show()\n",
        "df2.printSchema()\n",
        "logging.info('Calculated and added moving average of volume')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPSMlhwwxeYq",
        "outputId": "118a6ea5-2dc1-41cf-bacb-084597dfff28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------------+----------+-----+-----+-----+-----+---------+--------+------------------+---------------------+\n",
            "|Symbol|       Security_Name|      Date| Open| High|  Low|Close|Adj_Close|  Volume|    vol_moving_avg|adj_close_rolling_med|\n",
            "+------+--------------------+----------+-----+-----+-----+-----+---------+--------+------------------+---------------------+\n",
            "|   AAT|American Assets T...|2011-01-13|21.53| 22.0|21.18|21.25|16.332218|15536900|         1.55369E7|            16.332218|\n",
            "|   AAT|American Assets T...|2011-01-14|21.16|21.45|21.16|21.31|16.378332| 1304800|         8420850.0|            16.355274|\n",
            "|   AAT|American Assets T...|2011-01-18| 21.3|21.45| 21.2|21.37| 16.42445|  124800|         5655500.0|            16.378332|\n",
            "|   AAT|American Assets T...|2011-01-19|21.42|21.42|20.88|21.25|16.332218| 1010200|         4494175.0|            16.355274|\n",
            "|   AAT|American Assets T...|2011-01-20|21.05| 21.4|21.03|21.21|16.301474|  736600|         3742660.0|            16.332218|\n",
            "|   AAT|American Assets T...|2011-01-21| 21.3| 21.3|21.03|21.25|16.332218|  636800|3225016.6666666665|            16.332218|\n",
            "|   AAT|American Assets T...|2011-01-24| 21.2|21.39| 21.1| 21.3|16.370651|  354300| 2814914.285714286|            16.332218|\n",
            "|   AAT|American Assets T...|2011-01-25| 21.2|21.47|21.15|21.27|16.347591|  295700|         2500012.5|            16.339905|\n",
            "|   AAT|American Assets T...|2011-01-26|21.29|21.36|21.21|21.27|16.347591|  346800|2260766.6666666665|            16.347591|\n",
            "|   AAT|American Assets T...|2011-01-27| 21.2| 21.4| 21.2| 21.4|16.447508|  450400|         2079730.0|            16.347591|\n",
            "|   AAT|American Assets T...|2011-01-28|21.26|21.38|21.26| 21.3|16.370651|  475300|1933872.7272727273|            16.347591|\n",
            "|   AAT|American Assets T...|2011-01-31| 21.3| 21.4|21.25|21.29| 16.36296|  197600|1789183.3333333333|            16.355276|\n",
            "|   AAT|American Assets T...|2011-02-01|21.27| 21.4|21.21| 21.3|16.370651|  560900|         1694700.0|             16.36296|\n",
            "|   AAT|American Assets T...|2011-02-02|21.15|21.25|21.06|21.11|16.224619|  168300|1585671.4285714286|            16.355276|\n",
            "|   AAT|American Assets T...|2011-02-03|21.02|21.28|21.02|21.18| 16.27842|  549400|1516586.6666666667|            16.347591|\n",
            "|   AAT|American Assets T...|2011-02-04| 21.1|21.28| 21.0|21.06| 16.18619|  219600|         1435525.0|            16.347591|\n",
            "|   AAT|American Assets T...|2011-02-07|21.07| 21.2|20.95|21.16|16.263054|  454900|1377841.1764705882|            16.347591|\n",
            "|   AAT|American Assets T...|2011-02-08|21.21|21.21|20.94|20.95|16.101648|  247100|1315022.2222222222|            16.339905|\n",
            "|   AAT|American Assets T...|2011-02-09| 20.8| 21.2|20.61|21.14|16.247673|  423900|1268121.0526315789|            16.332218|\n",
            "|   AAT|American Assets T...|2011-02-10|21.01|21.35|21.01|21.24|16.324533|  256000|         1217515.0|            16.332218|\n",
            "+------+--------------------+----------+-----+-----+-----+-----+---------+--------+------------------+---------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "root\n",
            " |-- Symbol: string (nullable = false)\n",
            " |-- Security_Name: string (nullable = true)\n",
            " |-- Date: string (nullable = true)\n",
            " |-- Open: float (nullable = true)\n",
            " |-- High: float (nullable = true)\n",
            " |-- Low: float (nullable = true)\n",
            " |-- Close: float (nullable = true)\n",
            " |-- Adj_Close: float (nullable = true)\n",
            " |-- Volume: integer (nullable = true)\n",
            " |-- vol_moving_avg: double (nullable = true)\n",
            " |-- adj_close_rolling_med: float (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import udf, collect_list\n",
        "import numpy as np\n",
        "from pyspark.sql.types import FloatType\n",
        "\n",
        "median_udf = udf(lambda x: float(np.median(x)), FloatType())\n",
        "\n",
        "df2 = df2.withColumn(\"list\", collect_list(\"Adj_Close\").over(windowSpec)) \\\n",
        "  .withColumn(\"adj_close_rolling_med\", median_udf(\"list\"))\n",
        "\n",
        "df2 = df2.drop(\"list\")\n",
        "# Show the resulting DataFrame\n",
        "df2.show()\n",
        "df2.printSchema()\n",
        "logging.info('Calculated and added rolling median of Adj Close')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Vm2pDZeKbPT"
      },
      "source": [
        "#### df2 now contains the resulting dataset with new features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RfJNAUILcaV"
      },
      "outputs": [],
      "source": [
        "# Save as CSV\n",
        "# Try to use coalesce() instead of repartition() to reduce the number of partitions\n",
        "df2.repartition(1).write.csv(\"etfs2.csv\", header=True, mode=\"overwrite\")\n",
        "logging.info('Saved the new DataFrame as a CSV file')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "qQADAssXLper"
      },
      "outputs": [],
      "source": [
        "# Save as Parquet\n",
        "# Try to use coalesce() instead of repartition() to reduce the number of partitions\n",
        "df2.repartition(1).write.parquet(\"etfs2.parquet\", mode=\"overwrite\")\n",
        "logging.info('Saved the new DataFrame as a Parquet file')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqz0vColLzbP"
      },
      "source": [
        "### Integrate ML Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAm_JfKoQ7nt"
      },
      "source": [
        "#### Method 1: Use Scikit-learn ML model\n",
        "(Out of Memory Error)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8J2kel3AL8uZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "data_from_parquet = pd.read_parquet('/content/etfs2.parquet/part-00000-7836736d-c28e-4cf6-94c3-27cf3c804dd3-c000.snappy.parquet', columns=['vol_moving_avg', 'adj_close_rolling_med', 'Volume'])\n",
        "#data_from_csv = pd.read_csv('/content/etfs2.csv/etfs2.csv')\n",
        "logging.info('Read data from Parquet')\n",
        "data = data_from_parquet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "jDLzo53FMZLC",
        "outputId": "43403139-925f-4b20-839e-4870f90c95a0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-af82cedc-dde1-4b31-b6cf-f2fa82e4caed\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>vol_moving_avg</th>\n",
              "      <th>adj_close_rolling_med</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.044911</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>298200.000000</td>\n",
              "      <td>24.613100</td>\n",
              "      <td>298200.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>150700.000000</td>\n",
              "      <td>24.613100</td>\n",
              "      <td>3200.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>102733.333333</td>\n",
              "      <td>24.613100</td>\n",
              "      <td>6800.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>77625.000000</td>\n",
              "      <td>24.658989</td>\n",
              "      <td>2300.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-af82cedc-dde1-4b31-b6cf-f2fa82e4caed')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-af82cedc-dde1-4b31-b6cf-f2fa82e4caed button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-af82cedc-dde1-4b31-b6cf-f2fa82e4caed');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   vol_moving_avg  adj_close_rolling_med    Volume\n",
              "0        0.000000               0.044911       0.0\n",
              "1   298200.000000              24.613100  298200.0\n",
              "2   150700.000000              24.613100    3200.0\n",
              "3   102733.333333              24.613100    6800.0\n",
              "4    77625.000000              24.658989    2300.0"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_from_parquet.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLwXbPYnLw63"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "data = data_from_parquet\n",
        "#data['Date'] = pd.to_datetime(data['Date'])\n",
        "#data.set_index('Date', inplace=True)\n",
        "\n",
        "# Remove rows with NaN values\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Select features and target\n",
        "features = ['vol_moving_avg', 'adj_close_rolling_med']\n",
        "target = 'Volume'\n",
        "\n",
        "X = data[features]\n",
        "y = data[target]\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a RandomForestRegressor model\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\"\"\"\n",
        "Out of Memory\n",
        "\"\"\"\n",
        "\n",
        "# Make predictions on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the Mean Absolute Error and Mean Squared Error\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "logging.info('Trained and saved....')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5qUMfZMVvEp"
      },
      "outputs": [],
      "source": [
        "print(mae)\n",
        "print(mse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_GfR8LUQvjS"
      },
      "source": [
        "#### Method 2: Use Spark ML models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-XhH2MqB6sU",
        "outputId": "73570755-5772-469e-cc9a-7792442779dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------------+----------+-----+-----+-----+-----+---------+--------+------------------+---------------------+--------------------+\n",
            "|Symbol|       Security_Name|      Date| Open| High|  Low|Close|Adj_Close|  Volume|    vol_moving_avg|adj_close_rolling_med|            features|\n",
            "+------+--------------------+----------+-----+-----+-----+-----+---------+--------+------------------+---------------------+--------------------+\n",
            "|   AAT|American Assets T...|2011-01-13|21.53| 22.0|21.18|21.25|16.332218|15536900|         1.55369E7|            16.332218|[1.55369E7,16.332...|\n",
            "|   AAT|American Assets T...|2011-01-14|21.16|21.45|21.16|21.31|16.378332| 1304800|         8420850.0|            16.355274|[8420850.0,16.355...|\n",
            "|   AAT|American Assets T...|2011-01-18| 21.3|21.45| 21.2|21.37| 16.42445|  124800|         5655500.0|            16.378332|[5655500.0,16.378...|\n",
            "|   AAT|American Assets T...|2011-01-19|21.42|21.42|20.88|21.25|16.332218| 1010200|         4494175.0|            16.355274|[4494175.0,16.355...|\n",
            "|   AAT|American Assets T...|2011-01-20|21.05| 21.4|21.03|21.21|16.301474|  736600|         3742660.0|            16.332218|[3742660.0,16.332...|\n",
            "|   AAT|American Assets T...|2011-01-21| 21.3| 21.3|21.03|21.25|16.332218|  636800|3225016.6666666665|            16.332218|[3225016.66666666...|\n",
            "|   AAT|American Assets T...|2011-01-24| 21.2|21.39| 21.1| 21.3|16.370651|  354300| 2814914.285714286|            16.332218|[2814914.28571428...|\n",
            "|   AAT|American Assets T...|2011-01-25| 21.2|21.47|21.15|21.27|16.347591|  295700|         2500012.5|            16.339905|[2500012.5,16.339...|\n",
            "|   AAT|American Assets T...|2011-01-26|21.29|21.36|21.21|21.27|16.347591|  346800|2260766.6666666665|            16.347591|[2260766.66666666...|\n",
            "|   AAT|American Assets T...|2011-01-27| 21.2| 21.4| 21.2| 21.4|16.447508|  450400|         2079730.0|            16.347591|[2079730.0,16.347...|\n",
            "+------+--------------------+----------+-----+-----+-----+-----+---------+--------+------------------+---------------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create a SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"pipeline\").getOrCreate()\n",
        "\n",
        "# Read from Parquet\n",
        "#parDF=spark.read.parquet('/content/drive/MyDrive/RiskThinkingAI/etfs2.parquet')\n",
        "#parDF=spark.read.parquet('/content/etfs2.parquet/etfs_stocks_2.parquet')\n",
        "parDF = df2\n",
        "\n",
        "parDF = parDF.na.drop()\n",
        "# Reference: https://hackernoon.com/building-a-machine-learning-model-with-pyspark-a-step-by-step-guide-1z2d3ycd\n",
        "\n",
        "required_features = ['vol_moving_avg', 'adj_close_rolling_med']\n",
        "\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "assembler = VectorAssembler(inputCols=required_features, outputCol='features')\n",
        "\n",
        "transformed_data = assembler.transform(parDF)\n",
        "transformed_data.show(10)\n",
        "transformed_data = transformed_data.select(['features', 'Volume'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "HhyDqp_DMixh"
      },
      "outputs": [],
      "source": [
        "# Split the data\n",
        "\n",
        "#(training_data, test_data) = transformed_data.randomSplit([0.8,0.2], seed =2020)\n",
        "#print(f\"Training Dataset Count: {training_data.count()}\")\n",
        "#print(f\"Test Dataset Count: {test_data.count()}\")\n",
        "\n",
        "splits = transformed_data.randomSplit([0.8, 0.2])\n",
        "training_data = splits[0]\n",
        "test_data = splits[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "h27Ph6TOMnkc",
        "outputId": "a076f710-8a5f-435c-b388-734523cfcafd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:KeyboardInterrupt while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-cb80d959cfed>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                       \u001b[0mfeaturesCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                       maxIter=10, regParam=0.3, elasticNetParam=0.8)\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mrf_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "lr = LinearRegression(labelCol='Volume', \n",
        "                      featuresCol='features',\n",
        "                      maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
        "model = lr.fit(training_data)\n",
        "rf_predictions = model.transform(test_data)\n",
        "\n",
        "print(\"Coefficients: \" + str(model.coefficients))\n",
        "print(\"Intercept: \" + str(model.intercept))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIwjw6RlMrAt"
      },
      "outputs": [],
      "source": [
        "# Summarize the model over the training set and print out some metrics\n",
        "trainingSummary = model.summary\n",
        "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
        "print(\"r2: %f\" % trainingSummary.r2)\n",
        "\n",
        "logging.info('Trained model using PySpark ML')\n",
        "logging.info(f'With RMSE: {trainingSummary.rootMeanSquaredError}')\n",
        "logging.info(f'With r2: {trainingSummary.r2}')\n",
        "logging.info(f'With loss: {model.getLoss()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "4304jhluXPTm"
      },
      "outputs": [],
      "source": [
        "#from pyspark.context import SparkContext\n",
        "# save the model to disk\n",
        "filename = str(today) + '-lr-model'\n",
        "#joblib.dump(model, filename)\n",
        "model.save(filename)\n",
        "logging.info(f'Saved models as {filename}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "mPjMKUk1Mc0F"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.regression import LinearRegressionModel\n",
        "\n",
        "sameModel = LinearRegressionModel.load(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgMH8AZaOoOl",
        "outputId": "3fb8a0a0-2d0c-46fa-ac18-ec8d5327d700"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------+------------------+\n",
            "|            features|Volume|        prediction|\n",
            "+--------------------+------+------------------+\n",
            "|[0.0,-1999.244873...|     0|21559.669654760044|\n",
            "|[0.0,-1999.244873...|     0|21559.669654760044|\n",
            "|[0.0,-1293.955566...|     0|21559.669654760044|\n",
            "|[0.0,-1288.402099...|     0|21559.669654760044|\n",
            "|[0.0,-1288.402099...|     0|21559.669654760044|\n",
            "|[0.0,-1282.848999...|     0|21559.669654760044|\n",
            "|[0.0,-1227.314208...|     0|21559.669654760044|\n",
            "|[0.0,-1216.207519...|     0|21559.669654760044|\n",
            "|[0.0,-1216.207519...|     0|21559.669654760044|\n",
            "|[0.0,-1207.877197...|     0|21559.669654760044|\n",
            "+--------------------+------+------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "rf_predictions = sameModel.transform(test_data)\n",
        "rf_predictions.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXKSkaohTPAQ",
        "outputId": "4a85a9e3-4a13-41d0-91b3-7515a695ca0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+------+------------------+\n",
            "|            features|Volume|        prediction|\n",
            "+--------------------+------+------------------+\n",
            "|[0.0,-1771.553222...|     0| 17621.29583576283|\n",
            "|[0.0,-1732.678955...|     0| 17621.28854090555|\n",
            "|[0.0,-1574.405395...|     0| 17621.25884046099|\n",
            "|[0.0,-1524.424316...|     0|17621.249461381674|\n",
            "|[0.0,-1293.955566...|     0| 17621.20621332211|\n",
            "|[0.0,-1288.402099...|     0|17621.205171199643|\n",
            "|[0.0,-1288.402099...|     0|17621.205171199643|\n",
            "|[0.0,-1288.402099...|     0|17621.205171199643|\n",
            "|[0.0,-1282.848999...|     0| 17621.20412914589|\n",
            "|[0.0,-1238.421142...|     0|17621.195792143233|\n",
            "+--------------------+------+------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sameModelFromDrive = LinearRegressionModel.load(\"/content/drive/MyDrive/StockMarket/2023-05-07-lr-model\")\n",
        "\n",
        "rf_predictions = sameModelFromDrive.transform(test_data)\n",
        "rf_predictions.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze"
      ],
      "metadata": {
        "id": "DowQF5TPut61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFYW7LISUEDv"
      },
      "source": [
        "### Problem 4. Model Serving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lutwsTsBTh-R",
        "outputId": "3b3e9390-7d50-41aa-9f72-2899bc7dd70d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+---------------------+\n",
            "|vol_moving_avg|adj_close_rolling_med|\n",
            "+--------------+---------------------+\n",
            "|         41431|            -5124.312|\n",
            "+--------------+---------------------+\n",
            "\n",
            "+-------------------+\n",
            "|           features|\n",
            "+-------------------+\n",
            "|[41431.0,-5124.312]|\n",
            "+-------------------+\n",
            "\n",
            "+------------------+\n",
            "|        prediction|\n",
            "+------------------+\n",
            "|62171.124431635784|\n",
            "+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#from pyspark.sql.functions import concat\n",
        "apiData = spark.createDataFrame([\n",
        "    (41431, -5124.312)], \n",
        "    [\"vol_moving_avg\", \"adj_close_rolling_med\"])\n",
        "\n",
        "apiData.show()\n",
        "\n",
        "'''\n",
        "# combine the three columns into a single column named \"features\"\n",
        "apiData = apiData.withColumn(\"features\", concat(col(\"vol_moving_avg\"), col(\"vol_moving_avg\")))\n",
        "apiData = transformed_data.select(['features'])\n",
        "apiData.show()\n",
        "'''\n",
        "\n",
        "apiDataTransformed = assembler.transform(apiData)\n",
        "apiDataTransformed = apiDataTransformed.select(['features'])\n",
        "apiDataTransformed.show()\n",
        "\n",
        "apiDataPredictions = sameModel.transform(apiDataTransformed)\n",
        "apiDataPredictions.select('prediction').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNaRbnGcdLIK",
        "outputId": "425160cf-8f65-4045-d13f-eab96cb257d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "58796.478232862195"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "apiDataPredictions.collect()[0][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2xTBGh1hxI2"
      },
      "outputs": [],
      "source": [
        "#!pip install flask-ngrok # Use only when the notebook is running on Colab\n",
        "# https://www.geeksforgeeks.org/how-to-run-flask-app-on-google-colab/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "_ynPZzCwZQNu",
        "outputId": "bb52584f-27cb-45e2-af5e-b9a88841652b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://q5h1pm05px-496ff2e9c6d22116-5000-colab.googleusercontent.com/\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [09/May/2023 05:30:44] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [09/May/2023 05:30:44] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [09/May/2023 05:32:00] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [09/May/2023 05:32:00] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"
          ]
        }
      ],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from pyspark.ml.regression import LinearRegressionModel\n",
        "#import pandas as pd\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"api\").getOrCreate()\n",
        "\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "required_features = ['vol_moving_avg', 'adj_close_rolling_med']\n",
        "assembler = VectorAssembler(inputCols=required_features, outputCol='features')\n",
        "\n",
        "\n",
        "\n",
        "from google.colab.output import eval_js\n",
        "print(eval_js(\"google.colab.kernel.proxyPort(5000)\"))\n",
        "\n",
        "# https://brightersidetech.com/running-flask-apps-in-google-colab/\n",
        "    \n",
        "app = Flask(__name__)\n",
        "model = LinearRegressionModel.load(\"/content/drive/MyDrive/StockMarket/2023-05-07-lr-model\")\n",
        "\n",
        "def volume_prediction(model, data):\n",
        "    #df = pd.DataFrame(data=data)\n",
        "\n",
        "    apiData = spark.createDataFrame([data], \n",
        "    required_features)\n",
        "\n",
        "    apiData = assembler.transform(apiData)\n",
        "    apiData = apiData.select(['features'])\n",
        "\n",
        "    prediction = model.transform(apiData)\n",
        "    # https://www.geeksforgeeks.org/get-value-of-a-particular-cell-in-pyspark-dataframe/\n",
        "    #print(f'Prediction: {int(prediction.collect()[0][1])}')\n",
        "    return {'volume': int(prediction.collect()[0][1])}\n",
        "\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "    return \"Hello World\"\n",
        "\n",
        "\"\"\"\n",
        "@app.route('/test')\n",
        "def test():\n",
        "  return jsonify({'test': 'You can access test API'}), 200\n",
        "\"\"\"\n",
        "@app.route('/predict')\n",
        "def get_volume():\n",
        "    vol_moving_avg = request.args.get('vol_moving_avg')\n",
        "    adj_close_rolling_med = request.args.get('adj_close_rolling_med')\n",
        "    \n",
        "    if not vol_moving_avg or not adj_close_rolling_med:\n",
        "        return jsonify({'error': 'You need to supply both vol_moving_avg and adj_close_rolling_med'}), 400\n",
        "\n",
        "    data = [float(vol_moving_avg), float(adj_close_rolling_med)]\n",
        "    return jsonify({\n",
        "        **volume_prediction(model, data),\n",
        "    })\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_8xN-Jqyed-o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc2b870c-2145-4677-9167-2b5c04183b1b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Response [404]>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "import requests\n",
        "volume = requests.get('https://q5h1pm05px-496ff2e9c6d22116-5000-colab.googleusercontent.com/predict', \n",
        "                       params={'vol_moving_avg': 41431, 'adj_close_rolling_med':-5124.312})\n",
        "volume"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97-KMcQWTTxj"
      },
      "source": [
        "#### Download a folder by zipping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "252CFClxOPig",
        "outputId": "d211305b-30a3-4752-9e3c-0566cb4344ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/2023-05-08-lr-model/ (stored 0%)\n",
            "  adding: content/2023-05-08-lr-model/metadata/ (stored 0%)\n",
            "  adding: content/2023-05-08-lr-model/metadata/_SUCCESS (stored 0%)\n",
            "  adding: content/2023-05-08-lr-model/metadata/.part-00000.crc (stored 0%)\n",
            "  adding: content/2023-05-08-lr-model/metadata/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/2023-05-08-lr-model/metadata/part-00000 (deflated 44%)\n",
            "  adding: content/2023-05-08-lr-model/data/ (stored 0%)\n",
            "  adding: content/2023-05-08-lr-model/data/_SUCCESS (stored 0%)\n",
            "  adding: content/2023-05-08-lr-model/data/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/2023-05-08-lr-model/data/part-00000-89105cda-1d70-4650-8fc8-bf91d3e8b8a4-c000.snappy.parquet (deflated 55%)\n",
            "  adding: content/2023-05-08-lr-model/data/.part-00000-89105cda-1d70-4650-8fc8-bf91d3e8b8a4-c000.snappy.parquet.crc (stored 0%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_edcbcfd5-f97f-43c3-b4f4-a1d71b185009\", \"model.zip\", 3922)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "!zip -r /content/model.zip /content/2023-05-08-lr-model\n",
        "from google.colab import files\n",
        "files.download(\"/content/model.zip\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "CAm_JfKoQ7nt"
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}